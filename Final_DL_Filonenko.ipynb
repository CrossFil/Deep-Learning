{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ПОЯСНЕННЯ ЩОДО ВИКОНАННЯ\n",
        "\n",
        "Як було обговорено на практичному занняті, працювати на лімітованому GPU дуже важко. Це завдання робив тиждень, майже не відходячи від тренування моделі, бо після закінчення GPU, проводив налаштування на CPU."
      ],
      "metadata": {
        "id": "0GjlFkDkmvk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rawOeJXvsFXn",
        "outputId": "d1a903eb-6f23-4afd-f85e-b380f0419022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAeqXq-mtIHe",
        "outputId": "0709b01f-e7b3-4467-99bc-bf83d8631b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.0+cu121\n",
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jllV0B2wC8Pe",
        "outputId": "08c08c42-b6b8-45f0-9200-21055eb0e653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9qkouOaeA2_m",
        "outputId": "90606dcd-f0f8-47dd-c431-e9dff3cc00df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b619c14-1fa5-49e8-a6ae-07ba478d20e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7b619c14-1fa5-49e8-a6ae-07ba478d20e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"crossfil\",\"key\":\"fc987fb42b575d745b04d3d86099793f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM1q4dQH_n60"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jnAYwhD_iU-",
        "outputId": "5fd74268-6f3f-4409-da4c-eb07433f3387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading deep-learning-for-computer-vision-and-nlp-2025-02.zip to /content\n",
            " 99% 989M/997M [00:13<00:00, 87.3MB/s]\n",
            "100% 997M/997M [00:13<00:00, 76.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c deep-learning-for-computer-vision-and-nlp-2025-02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8w9ZYLOBBgU"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('deep-learning-for-computer-vision-and-nlp-2025-02.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KiIXKQM87-G-",
        "outputId": "ceff480c-147f-48fb-e82d-6e118d1fb29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6431 entries, 0 to 6430\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   PetID          6431 non-null   object\n",
            " 1   Description    6426 non-null   object\n",
            " 2   AdoptionSpeed  6431 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 150.9+ KB\n",
            "None\n",
            "\n",
            "Train DataFrame Head:\n",
            "       PetID                                        Description  AdoptionSpeed\n",
            "0  d3b4f29f8  Mayleen and Flo are two lovely adorable sister...              2\n",
            "1  e9dc82251  A total of 5 beautiful Tabbys available for ad...              2\n",
            "2  8111f6d4a  Two-and-a-half month old girl. Very manja and ...              2\n",
            "3  693a90fda  Neil is a healthy and active ~2-month-old fema...              2\n",
            "4  9d08c85ef  Gray kitten available for adoption in sungai p...              2\n",
            "\n",
            "Test DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1891 entries, 0 to 1890\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   PetID        1891 non-null   object\n",
            " 1   Description  1890 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 29.7+ KB\n",
            "None\n",
            "\n",
            "Test DataFrame Head:\n",
            "       PetID                                        Description\n",
            "0  6697a7f62  This cute little puppy is looking for a loving...\n",
            "1  23b64fe21  These 3 puppies was rescued from a mechanic sh...\n",
            "2  41e824cbe  Ara needs a forever home! Believe me, he's a r...\n",
            "3  6c3d7237b  i rescue this homeless dog 2 years ago but my ...\n",
            "4  97b0b5d92  We found him at a shopping mall at a very clea...\n",
            "\n",
            "Missing values in train set:\n",
            "PetID            0\n",
            "Description      5\n",
            "AdoptionSpeed    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Завантаження\n",
        "DATA_DIR = 'data'\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
        "sample_submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
        "\n",
        "# Перевірка структури даних\n",
        "print(\"Train DataFrame Info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nTrain DataFrame Head:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest DataFrame Info:\")\n",
        "print(test_df.info())\n",
        "print(\"\\nTest DataFrame Head:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Перевірка наявності пропущених значень\n",
        "print(\"\\nMissing values in train set:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# Розподіл цільової змінної (для аналізу балансу класів)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='AdoptionSpeed', data=train_df)\n",
        "plt.title('Distribution of Target Variable')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gn05oCPO9gxh",
        "outputId": "20177954-4c11-45ac-e4dc-172c884e0e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp311-cp311-linux_x86_64.whl (757.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting jinja2 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from torchvision==0.17.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.17.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.17.0\n",
            "    Uninstalling filelock-3.17.0:\n",
            "      Successfully uninstalled filelock-3.17.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\n",
            "pytensor 2.27.1 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.1.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 sympy-1.13.1 torch-2.2.2+cu121 torchaudio-2.2.2+cu121 torchvision-0.17.2+cu121 triton-2.2.0 typing-extensions-4.12.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "140f64e844304dedbcaaf0158a029ada",
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall\n",
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gGpo3Q6Q_2pJ",
        "outputId": "e3969315-1214-4d20-9eae-112cb67107c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.17.0\n",
            "  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from torchtext==0.17.0)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from torchtext==0.17.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.2.0 (from torchtext==0.17.0)\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m822.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from torchtext==0.17.0)\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests->torchtext==0.17.0)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->torchtext==0.17.0)\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->torchtext==0.17.0)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->torchtext==0.17.0)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting sympy (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, charset-normalizer, idna, urllib3, certifi, requests, filelock, typing-extensions, mpmath, sympy, networkx, MarkupSafe, jinja2, fsspec, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, triton, torch, numpy, torchdata, torchtext\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2+cu121\n",
            "    Uninstalling torch-2.2.2+cu121:\n",
            "      Successfully uninstalled torch-2.2.2+cu121\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.2+cu121 requires torch==2.2.2, but you'll have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.17.2+cu121 requires torch==2.2.2, but you'll have torch 2.2.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you'll have numpy 2.2.3 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you'll have numpy 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you'll have numpy 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you'll have numpy 2.2.3 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you'll have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.0 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 requests-2.32.3 sympy-1.13.3 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 tqdm-4.67.1 triton-2.2.0 typing-extensions-4.12.2 urllib3-2.3.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "c3a1de03808e4d42afc6bc3cf34cd5da",
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torchtext==0.17.0 --use-deprecated=legacy-resolver --force-reinstall\n",
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "collapsed": true,
        "id": "rgtHM7P0_7zc",
        "outputId": "20f063f9-b2d6-4754-b8e2-ffb626270460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.3\n",
            "    Uninstalling numpy-2.2.3:\n",
            "      Successfully uninstalled numpy-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.17.2+cu121 requires torch==2.2.2, but you have torch 2.2.0 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "526d336d8dd84586bf4a1f1d8993ec1f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install numpy==1.25.2\n",
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b972c965d161462594e077f9c6761ca9",
            "de29152d70624055a4ce5a6f61caa3bb",
            "93a3985f99a84be1a82ad466e0f61570",
            "bc416ae258874d63b05204ccee2546a6",
            "f41814a778be4b969f9fdf208d898f86",
            "b93de66703c545c7adeaa2b43c4df563",
            "5c79c83aa8ce4a128f7c4202085410c1",
            "998020b7723940e58d887b6399722676",
            "00d55b560b984167bf4ec282372d59c4",
            "39ee7ed0aa014e12b9c8666b74e7b710",
            "c982ed59731e4c2eafed3fc153eeafb7",
            "895782f530654dcda16e604498a52517",
            "f83f0005bf4a4d0085fa0997de1e2531",
            "756637439b7b4497b8163cc55e5d0aea",
            "6fe91a0558984c39a6d500c506c8627a",
            "7af200a4dbc44cb0b3c9f90fe6769706",
            "82d296d26bfe4cb694ca8e26fa92b181",
            "9419235cff00446d90391ca14261a4b8",
            "48aa82fd75474efd9d6a8cebbe38c03a",
            "65b66750977a4fc79c82e3d2e07f8a08",
            "00b1ee238fc944abb6fa479311e66b51",
            "36b9244ca57846b5ace54f9fbc63a7e4"
          ]
        },
        "id": "j3xldP5vEW0m",
        "outputId": "53e286b6-7463-4161-fa60-41fbb83705d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 170MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b972c965d161462594e077f9c6761ca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "895782f530654dcda16e604498a52517"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PetAdoptionModel(\n",
            "  (resnet): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (image_fc): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (text_fc): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import BertModel\n",
        "\n",
        "class PetAdoptionModel(nn.Module):\n",
        "    def __init__(self, num_classes=4, bert_model_name='bert-base-uncased'):\n",
        "        super(PetAdoptionModel, self).__init__()\n",
        "\n",
        "        # ResNet50 для зображень\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        resnet.fc = nn.Identity()  # Видаляємо останній шар\n",
        "        self.resnet = resnet\n",
        "        self.image_fc = nn.Linear(2048, 128)  # Приводимо розмірність ознак зображення до 128\n",
        "\n",
        "        # BERT для тексту\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.text_feature_dim = self.bert.config.hidden_size\n",
        "        # Зменшуємо розмірність текстових ознак до 128\n",
        "        self.text_fc = nn.Linear(self.text_feature_dim, 128)\n",
        "\n",
        "        # Фінальний класифікатор: об'єднання ознак з зображення та тексту\n",
        "        self.classifier = nn.Linear(128 + 128, num_classes)\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Обробка зображень через ResNet\n",
        "        # Обмежуємо кількість фото до максимум 5\n",
        "        if images.dim() == 5:  # Перевіряємо, чи вхід має форму [batch_size, num_images, C, H, W]\n",
        "           images = images[:, :5]  # Беремо максимум 5 фото\n",
        "           batch_size, num_images, C, H, W = images.size()\n",
        "           images = images.view(batch_size * num_images, C, H, W)  # Перетворюємо для ResNet\n",
        "\n",
        "           # Пропускаємо через ResNet\n",
        "           image_features = self.resnet(images)  # [batch_size * num_images, 2048]\n",
        "           image_features = image_features.view(batch_size, num_images, -1)  # [batch_size, num_images, 2048]\n",
        "\n",
        "            # Усереднюємо ознаки по фото\n",
        "           image_features = torch.mean(image_features, dim=1)  # [batch_size, 2048]\n",
        "        else:\n",
        "           image_features = self.resnet(images)\n",
        "\n",
        "\n",
        "        # Обробка тексту через BERT\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Використовуємо вектор [CLS] як представлення тексту\n",
        "        text_features = bert_output.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
        "        text_features = self.text_fc(text_features)             # [batch_size, 128]\n",
        "\n",
        "        # Об'єднання ознак\n",
        "        combined = torch.cat((image_features, text_features), dim=1)  # [batch_size, 256]\n",
        "\n",
        "        # Фінальний прогноз\n",
        "        output = self.classifier(combined)                        # [batch_size, num_classes]\n",
        "        return output\n",
        "\n",
        "# Ініціалізація моделі\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PetAdoptionModel(num_classes=4).to(device)\n",
        "\n",
        "# Вибір оптимізатора та функції втрат\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Вивід архітектури моделі\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# ------------------------- Параметри -------------------------\n",
        "MAX_TEXT_LENGTH = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# ------------------------- Завантаження даних -------------------------\n",
        "data_dir = '/content/data/'\n",
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
        "\n",
        "# ------------------------- Попередня обробка тексту -------------------------\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return \"No description available\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Видалити пунктуацію\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()    # Видалити зайві пробіли\n",
        "    return text\n",
        "\n",
        "\n",
        "train_df['Description'] = train_df['Description'].apply(clean_text)\n",
        "test_df['Description'] = test_df['Description'].apply(clean_text)\n",
        "\n",
        "# ------------------------- Ініціалізація BERT-токенізатора -------------------------\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_text(text, max_length=MAX_TEXT_LENGTH):\n",
        "    encoding = bert_tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoding['input_ids'][0], encoding['attention_mask'][0]\n",
        "\n",
        "\n",
        "# Попереднє токенізування текстів для тренувального і тестового наборів\n",
        "train_text_data = [tokenize_text(text) for text in train_df['Description']]\n",
        "test_text_data = [tokenize_text(text) for text in test_df['Description']]\n",
        "\n",
        "# ------------------------- Обробка міток -------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['AdoptionSpeed'] = label_encoder.fit_transform(train_df['AdoptionSpeed'])\n",
        "\n",
        "# ------------------------- Побудова мапи зображень -------------------------\n",
        "image_map = {}\n",
        "image_dir = os.path.join(data_dir, 'images', 'images')\n",
        "\n",
        "for folder in ['train', 'test']:\n",
        "    folder_path = os.path.join(image_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if '-' in filename:  # Check if filename has a hyphen\n",
        "                pet_id = filename.split('-')[0]\n",
        "                # Додаємо всі зображення для одного PetID (до 5 штук)\n",
        "                if pet_id not in image_map:\n",
        "                    image_map[pet_id] = []\n",
        "                if len(image_map[pet_id]) < 5:\n",
        "                    image_map[pet_id].append(os.path.join(folder_path, filename))\n",
        "            else:\n",
        "                print(f\"Skipping file with invalid format: {filename}\")\n",
        "\n",
        "\n",
        "# ------------------------- Трансформації для зображень -------------------------\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ------------------------- Створення Dataset -------------------------\n",
        "class PetDataset(Dataset):\n",
        "    def __init__(self, dataframe, text_data, image_dir=None, transform=None, is_train=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.text_data = text_data\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx, row in dataframe.iterrows():\n",
        "            pet_id = row['PetID']\n",
        "            if pet_id in image_map and image_map[pet_id]:\n",
        "                self.valid_indices.append(idx)\n",
        "            else:\n",
        "                print(f\"Warning: No image found for PetID {pet_id}, skipping this data point.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.valid_indices[idx]\n",
        "        row = self.dataframe.iloc[real_idx]\n",
        "        pet_id = row['PetID']\n",
        "\n",
        "        if self.image_dir:\n",
        "            image_paths = []\n",
        "            # Шукаємо всі зображення для поточного PetID\n",
        "            for subfolder in ['train', 'test']:\n",
        "                for filename in image_map.get(pet_id, []):\n",
        "                    image_paths.append(filename)\n",
        "\n",
        "            if not image_paths:\n",
        "                print(f\"No image found for PetID {pet_id}, skipping\")\n",
        "                return None\n",
        "\n",
        "            try:\n",
        "                # Завантажуємо зображення\n",
        "                image = Image.open(image_paths[0]).convert('RGB')\n",
        "                if self.transform:\n",
        "                    image = self.transform(image)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {image_paths[0]}: {e}, skipping\")\n",
        "                return None\n",
        "\n",
        "            # Перевіряємо текстові дані\n",
        "            text = self.dataframe.iloc[real_idx]['Description']\n",
        "            if pd.isna(text) or text == \"\":\n",
        "                print(f\"Missing or empty text for PetID {pet_id}, skipping\")\n",
        "                return None\n",
        "\n",
        "            input_ids, attention_mask = tokenize_text(text)\n",
        "\n",
        "            label = torch.tensor(row['AdoptionSpeed'], dtype=torch.long) if self.is_train else -1\n",
        "\n",
        "            return image, input_ids, attention_mask, label\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "# ------------------------- Функція для фільтрації None -------------------------\n",
        "def custom_collate(batch):\n",
        "    batch = [data for data in batch if data is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    images, input_ids_list, attention_mask_list, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_mask = torch.stack(attention_mask_list)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return images, input_ids, attention_mask, labels\n",
        "\n",
        "# ------------------------- Розділення на тренувальний та валідаційний набори -------------------------\n",
        "train_df, valid_df, train_text_data, valid_text_data = train_test_split(\n",
        "    train_df, train_text_data, test_size=0.2, random_state=42, stratify=train_df['AdoptionSpeed']\n",
        ")\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "\n",
        "# ------------------------- Створення тренувального та валідаційного датасетів -------------------------\n",
        "train_dataset = PetDataset(train_df, train_text_data, image_dir=image_dir, is_train=True, transform=image_transforms)\n",
        "valid_dataset = PetDataset(valid_df, valid_text_data, image_dir=image_dir, is_train=True, transform=image_transforms)\n",
        "\n",
        "# ------------------------- DataLoader -------------------------\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "# ------------------------- Перевірка DataLoader -------------------------\n",
        "for batch in train_loader:\n",
        "    if batch is None:\n",
        "        print(\"Empty batch — skipping\")\n",
        "        continue\n",
        "    images, input_ids, attention_mask, labels = batch\n",
        "    print(\"Shape of images:\", images.shape)             # [batch_size, 3, 224, 224]\n",
        "    print(\"Shape of input_ids:\", input_ids.shape)       # [batch_size, MAX_TEXT_LENGTH]\n",
        "    print(\"Shape of attention_mask:\", attention_mask.shape)  # [batch_size, MAX_TEXT_LENGTH]\n",
        "    print(\"Shape of labels:\", labels.shape)             # [batch_size]\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "54b2a3ed140f450982f518ee91912fc2",
            "d75714cb24874195bfd368a08d925f63",
            "390405f67dcf406f92071f9239d29d3f",
            "ca4a845fffdc4610b7f8e6f38a7dc159",
            "ab0e4a69964549f5a416cf4b90cd272b",
            "e45000f4706e40caae982e99f9bc4f7d",
            "70eb67e4003c4328a9683b07730e303c",
            "660bdc8c0b1f4b51986e3396face4168",
            "f6c0f155b02a425f95437fa30beafbb5",
            "eeb14507576b4d71ad5e0d490f51f757",
            "c5ce1a9681a24570b94bda70c5e93790",
            "1df5b7383e4d4bf493ea69e3238c92a5",
            "7969cdbd914c4824af40a553270f146d",
            "1f891850eb0c40d88160fc6eca0a8078",
            "b38d26b070734be1b773f82cf005571b",
            "56eea96767e647afb4e2b735b86f449f",
            "8c36e585b9e24ce1a824035e9da9e4b2",
            "8d373867a71d4ab2b746e570c2aca66a",
            "69ea8eb9e817457885fa1e58249b665b",
            "be29716d5d874214a0a1ee481611a851",
            "b1ba63bd79b94bfeba89c14d128ea08e",
            "c203d0a55daf4f0c8ac8678acb8f84e4",
            "9f27a5e6c80747838cd679e69f550032",
            "6a39c17c0a6848bca4d39bbdc8dc3085",
            "643424c5a3f6489189273bffcd3489fe",
            "1bc5fd00cdad4df5ae36fc5c02ec53cc",
            "001a242e68734b4fbe442b668cf7c061",
            "4102314cd9414d09b8db882a09fa0294",
            "bf9f9f25a3c1428ba148d3c50f506bbe",
            "f9f6b6b19f0c4bf5bdab4daab03b5c0d",
            "876c750b07814f43b238d086b78ffe05",
            "6a64505bb5a74aadb55cfe1fb245ab3b",
            "85cd4006635449ae86912a8f9f36dedd"
          ]
        },
        "id": "JGL4bUtYmhZ0",
        "outputId": "6d89d949-c851-4f07-9dc9-49a19363f717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54b2a3ed140f450982f518ee91912fc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1df5b7383e4d4bf493ea69e3238c92a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f27a5e6c80747838cd679e69f550032"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of images: torch.Size([32, 3, 224, 224])\n",
            "Shape of input_ids: torch.Size([32, 100])\n",
            "Shape of attention_mask: torch.Size([32, 100])\n",
            "Shape of labels: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO17u1p7-sXl",
        "outputId": "f3bc7aab-01f3-48db-d90b-01bc53d16a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PetAdoptionModel(\n",
            "  (resnet): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (image_fc): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (combined_layer): Sequential(\n",
            "    (0): Linear(in_features=896, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.6, inplace=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.6, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class PetAdoptionModel(nn.Module):\n",
        "    def __init__(self, num_classes, bert_model_name='bert-base-uncased', hidden_size=768, num_attention_heads=8, num_hidden_layers=6):\n",
        "        super(PetAdoptionModel, self).__init__()\n",
        "\n",
        "        # ResNet50 для зображень\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        resnet.fc = nn.Identity()  # Видаляємо останній шар\n",
        "        self.resnet = resnet\n",
        "        self.image_feature_dim = 2048\n",
        "\n",
        "        # Приводимо ознаки зображення до 128 вимірів\n",
        "        self.image_fc = nn.Linear(self.image_feature_dim, 128)\n",
        "\n",
        "        # BERT для тексту\n",
        "        bert_config = BertConfig.from_pretrained(\n",
        "            bert_model_name,\n",
        "            hidden_size=hidden_size,\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            num_hidden_layers=num_hidden_layers\n",
        "        )\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name, config=bert_config)\n",
        "        self.text_feature_dim = hidden_size\n",
        "\n",
        "        # Додатковий нелінійний шар для об'єднання ознак\n",
        "        combined_input_dim = 128 + self.text_feature_dim\n",
        "        self.combined_layer = nn.Sequential(\n",
        "            nn.Linear(combined_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Обробка зображень через ResNet\n",
        "        image_features = self.resnet(images)            # [batch_size, 2048]\n",
        "        image_features = self.image_fc(image_features)    # [batch_size, 128]\n",
        "\n",
        "        # Обробка тексту через BERT (використовуємо вектор [CLS])\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = bert_output.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        # Об'єднання ознак\n",
        "        combined = torch.cat((image_features, text_features), dim=1)  # [batch_size, 128 + hidden_size]\n",
        "        combined = self.combined_layer(combined)  # [batch_size, 128]\n",
        "\n",
        "        # Фінальний прогноз\n",
        "        output = self.classifier(combined)  # [batch_size, num_classes]\n",
        "        return output\n",
        "\n",
        "# Приклад ініціалізації моделі:\n",
        "NUM_CLASSES = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PetAdoptionModel(num_classes=NUM_CLASSES, hidden_size=768).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yLilYR7Uyrf",
        "outputId": "1d792999-f07d-4809-95f9-29facec00734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN8KgqNy9O9r",
        "outputId": "6e01afff-01c1-49ec-93e9-90feed94f0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [1/10], Train Loss: 1.3721, Train QWK: 0.1376, Validation Loss: 1.3492, Validation QWK: 0.2731\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [2/10], Train Loss: 1.3145, Train QWK: 0.3448, Validation Loss: 1.3155, Validation QWK: 0.3118\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [3/10], Train Loss: 1.2151, Train QWK: 0.5176, Validation Loss: 1.3490, Validation QWK: 0.3132\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [4/10], Train Loss: 1.0499, Train QWK: 0.6792, Validation Loss: 1.3592, Validation QWK: 0.3004\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [5/10], Train Loss: 0.8609, Train QWK: 0.7925, Validation Loss: 1.4853, Validation QWK: 0.3695\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [6/10], Train Loss: 0.6860, Train QWK: 0.8561, Validation Loss: 1.6281, Validation QWK: 0.3416\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 983160271, skipping\n",
            "Epoch [7/10], Train Loss: 0.5589, Train QWK: 0.8934, Validation Loss: 1.7645, Validation QWK: 0.3090\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping triggered!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                       classes=np.unique(train_df['AdoptionSpeed']),\n",
        "                                       y=train_df['AdoptionSpeed'])\n",
        "\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# ------------------------- QWK метрика -------------------------\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "\n",
        "# ------------------------- EarlyStopping -------------------------\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=2, delta=0.001, path='best_model.pth'):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, valid_kappa, model):\n",
        "        score = valid_kappa\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        print(f\"Validation QWK improved. Saving model to {self.path}\")\n",
        "\n",
        "# ------------------------- Налаштування моделі -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Встановлюємо функцію втрат CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.resnet.parameters(), 'weight_decay': 1e-3},\n",
        "    {'params': model.bert.parameters(), 'weight_decay': 1e-5},\n",
        "    {'params': model.classifier.parameters(), 'weight_decay': 1e-5}\n",
        "], lr=1e-4)\n",
        "\n",
        "early_stopping = EarlyStopping(patience=2, delta=0.001, path='best_model.pth')\n",
        "\n",
        "# ------------------------- Тренувальний цикл -------------------------\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for images, input_ids, attention_mask, labels in train_loader:\n",
        "        if images is None or input_ids is None or attention_mask is None:\n",
        "            continue\n",
        "\n",
        "\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device) # Changed here to include input_ids & attention_mask\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    # Обчислюємо QWK для тренувального набору\n",
        "    train_kappa = quadratic_weighted_kappa(all_labels, all_preds)\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # ------------------------- Валідація -------------------------\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_labels = []\n",
        "    valid_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in valid_loader:\n",
        "            if images is None or input_ids is None or attention_mask is None or labels is None:\n",
        "                continue\n",
        "\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "            valid_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "\n",
        "            # Обчислюємо QWK для валідаційного набору\n",
        "            valid_kappa = quadratic_weighted_kappa(valid_labels, valid_preds)\n",
        "            avg_valid_loss = valid_loss / len(valid_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train QWK: {train_kappa:.4f}, \"\n",
        "          f\"Validation Loss: {avg_valid_loss:.4f}, Validation QWK: {valid_kappa:.4f}\")\n",
        "\n",
        "    # ------------------------- Early Stopping -------------------------\n",
        "    early_stopping(valid_kappa, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHlm3qX8gwW-",
        "outputId": "899985a8-ca40-46f5-9f82-ed5196c8d1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m378.9/383.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/231.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install optuna --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J3LH0j8IlsT",
        "outputId": "259ac2c2-b752-44bc-b0a7-5b90e3d6aab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-17 19:58:17,030] A new study created in memory with name: no-name-cd26db0d-e2cb-4a6e-bac3-5014e7739434\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3663, Train QWK: 0.0805, Valid Loss: 1.3461, Valid QWK: 0.1970\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 2: Train Loss: 1.3259, Train QWK: 0.2256, Valid Loss: 1.3156, Valid QWK: 0.2544\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 3: Train Loss: 1.2970, Train QWK: 0.3188, Valid Loss: 1.3266, Valid QWK: 0.2598\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 4: Train Loss: 1.2705, Train QWK: 0.3635, Valid Loss: 1.3384, Valid QWK: 0.2313\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 5: Train Loss: 1.2475, Train QWK: 0.4155, Valid Loss: 1.2937, Valid QWK: 0.3005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-17 20:09:58,306] Trial 0 finished with value: 0.30052909056348187 and parameters: {'lr': 8.97994032191994e-05, 'batch_size': 16, 'weight_decay': 7.921736138736728e-05}. Best is trial 0 with value: 0.30052909056348187.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation QWK improved. Saving model to best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3651, Train QWK: 0.0726, Valid Loss: 1.3086, Valid QWK: 0.2500\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 2: Train Loss: 1.3253, Train QWK: 0.2401, Valid Loss: 1.3038, Valid QWK: 0.2615\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 3: Train Loss: 1.2807, Train QWK: 0.3573, Valid Loss: 1.3320, Valid QWK: 0.2936\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 4: Train Loss: 1.2572, Train QWK: 0.3714, Valid Loss: 1.2910, Valid QWK: 0.3202\n",
            "Validation QWK improved. Saving model to best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-17 20:20:45,993] Trial 1 finished with value: 0.25971694824223357 and parameters: {'lr': 0.00017971725576653222, 'batch_size': 64, 'weight_decay': 0.0006643018386301126}. Best is trial 0 with value: 0.30052909056348187.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 1.2135, Train QWK: 0.4339, Valid Loss: 1.3758, Valid QWK: 0.2597\n",
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3794, Train QWK: 0.0057, Valid Loss: 1.3640, Valid QWK: 0.0311\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 2: Train Loss: 1.3709, Train QWK: 0.0055, Valid Loss: 1.3668, Valid QWK: -0.0017\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 3: Train Loss: 1.3647, Train QWK: -0.0010, Valid Loss: 1.3589, Valid QWK: 0.0000\n",
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-17 20:29:26,095] Trial 2 finished with value: 0.0 and parameters: {'lr': 0.0004379084183844166, 'batch_size': 32, 'weight_decay': 1.906669100191771e-05}. Best is trial 0 with value: 0.30052909056348187.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 1.3651, Train QWK: 0.0014, Valid Loss: 1.3605, Valid QWK: 0.0000\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping triggered!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3680, Train QWK: 0.0805, Valid Loss: 1.3457, Valid QWK: 0.2258\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 2: Train Loss: 1.3523, Train QWK: 0.1440, Valid Loss: 1.3208, Valid QWK: 0.2697\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 3: Train Loss: 1.3307, Train QWK: 0.2228, Valid Loss: 1.3165, Valid QWK: 0.2775\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 4: Train Loss: 1.3069, Train QWK: 0.2957, Valid Loss: 1.3053, Valid QWK: 0.2593\n",
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-17 20:40:59,184] Trial 3 finished with value: 0.2594732028033113 and parameters: {'lr': 0.0001217393476712048, 'batch_size': 16, 'weight_decay': 0.00019964326673285778}. Best is trial 0 with value: 0.30052909056348187.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 1.2892, Train QWK: 0.3183, Valid Loss: 1.3172, Valid QWK: 0.2595\n",
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3709, Train QWK: 0.0548, Valid Loss: 1.3989, Valid QWK: 0.1007\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 2: Train Loss: 1.3487, Train QWK: 0.1237, Valid Loss: 1.3441, Valid QWK: 0.1746\n",
            "Validation QWK improved. Saving model to best_model.pth\n",
            "Epoch 3: Train Loss: 1.3413, Train QWK: 0.1888, Valid Loss: 1.3306, Valid QWK: 0.1824\n",
            "Validation QWK improved. Saving model to best_model.pth\n"
          ]
        }
      ],
      "source": [
        "# optuna робив на попередніх версіях, значного покращення не дало, але після неї закінчувалась сесія colab\n",
        "\n",
        "# import optuna\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# # ------------------------- Оптимізація з Optuna -------------------------\n",
        "# def objective(trial):\n",
        "#     # Підбір параметрів для BERT і загальної архітектури\n",
        "#     learning_rate = trial.suggest_float('lr', 5e-5, 5e-4, log=True)\n",
        "#     batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "#     weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
        "\n",
        "#     # Fix hidden_size to 768 to match 'bert-base-uncased'\n",
        "#     hidden_size = 768\n",
        "#     # hidden_size = trial.suggest_int('hidden_size', 256, 512, step=64)  # Оптимальний розмір шару\n",
        "\n",
        "#     # ------------------------- Ініціалізація моделі -------------------------\n",
        "#     model = PetAdoptionModel(\n",
        "#         hidden_size=hidden_size,\n",
        "#         num_attention_heads=8,  # Фіксоване значення для спрощення пошуку\n",
        "#         num_hidden_layers=6,    # Фіксоване значення для стабільності\n",
        "#         num_classes=NUM_CLASSES\n",
        "#     ).to(device)\n",
        "\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "#     criterion = torch.nn.CrossEntropyLoss()\n",
        "#     early_stopping = EarlyStopping(patience=3, delta=0.001, path='best_model.pth')\n",
        "\n",
        "#     # ------------------------- DataLoader -------------------------\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     # ------------------------- Тренування (5 епох для кращого підбору) -------------------------\n",
        "#     num_epochs = 5\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         all_labels = []\n",
        "#         all_preds = []\n",
        "\n",
        "#         for batch in train_loader:\n",
        "#             if batch is None:\n",
        "#                 continue\n",
        "#             images, input_ids, attention_mask, labels = batch\n",
        "#             images = images.to(device)\n",
        "#             input_ids = input_ids.to(device)\n",
        "#             attention_mask = attention_mask.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "#         train_qwk = cohen_kappa_score(all_labels, all_preds, weights=\"quadratic\")\n",
        "#         avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "#         # ------------------------- Оцінка на валідації -------------------------\n",
        "#         model.eval()\n",
        "#         valid_loss = 0.0\n",
        "#         valid_labels = []\n",
        "#         valid_preds = []\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             # Changed here to match valid_loader output\n",
        "#             for images, input_ids, attention_mask, labels in valid_loader:\n",
        "#                 if images is None or input_ids is None or attention_mask is None or labels is None:\n",
        "#                     continue\n",
        "#                 images = images.to(device)\n",
        "#                 input_ids = input_ids.to(device)\n",
        "#                 attention_mask = attention_mask.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "\n",
        "#                 outputs = model(images, input_ids, attention_mask)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#                 valid_loss += loss.item()\n",
        "\n",
        "#                 _, preds = torch.max(outputs, 1)\n",
        "#                 valid_labels.extend(labels.cpu().numpy())\n",
        "#                 valid_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "#         avg_valid_loss = valid_loss / len(valid_loader)\n",
        "#         valid_qwk = cohen_kappa_score(valid_labels, valid_preds, weights=\"quadratic\")\n",
        "\n",
        "#         print(f\"Epoch {epoch + 1}: \"\n",
        "#               f\"Train Loss: {avg_train_loss:.4f}, Train QWK: {train_qwk:.4f}, \"\n",
        "#               f\"Valid Loss: {avg_valid_loss:.4f}, Valid QWK: {valid_qwk:.4f}\")\n",
        "\n",
        "#         # ------------------------- EarlyStopping -------------------------\n",
        "#         early_stopping(valid_qwk, model)\n",
        "#         if early_stopping.early_stop:\n",
        "#             print(\"Early stopping triggered!\")\n",
        "#             break\n",
        "\n",
        "#         # Передача QWK для Optuna\n",
        "#         trial.report(valid_qwk, epoch)\n",
        "#         if trial.should_prune():\n",
        "#             raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "#     return valid_qwk\n",
        "\n",
        "# # ------------------------- Запуск Optuna -------------------------\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"maximize\",\n",
        "#     pruner=optuna.pruners.MedianPruner(n_warmup_steps=2)  # Прискорює пошук\n",
        "# )\n",
        "# study.optimize(objective, n_trials=10)\n",
        "\n",
        "# # ------------------------- Найкращі параметри -------------------------\n",
        "# best_params = study.best_params\n",
        "# print(\"Best params:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# ------------------------- Поєднуємо train + valid у фінальний набір -------------------------\n",
        "final_dataset = PetDataset(\n",
        "    train_df,\n",
        "    train_df['Description'],\n",
        "    '/content/data/images/images',\n",
        "    transform=image_transforms\n",
        ")\n",
        "\n",
        "# Фільтруємо None із final_dataset (якщо текст або зображення відсутні)\n",
        "final_dataset = [data for data in final_dataset if data is not None]\n",
        "\n",
        "if not final_dataset:\n",
        "    raise ValueError(\"final_dataset is empty after filtering. Check if all images and texts are valid.\")\n",
        "\n",
        "# Створюємо DataLoader після фільтрації\n",
        "final_loader = torch.utils.data.DataLoader(\n",
        "    final_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# ------------------------- Ініціалізація моделі -------------------------\n",
        "hidden_size = 768  # Зафіксований розмір для 'bert-base-uncased'\n",
        "num_attention_heads = 8  # Залишаємо значення за замовчуванням для спрощення\n",
        "num_hidden_layers = 6  # Для стабільності вибрано значення по замовчуванню\n",
        "\n",
        "model = PetAdoptionModel(\n",
        "    hidden_size=hidden_size,\n",
        "    num_attention_heads=num_attention_heads,\n",
        "    num_hidden_layers=num_hidden_layers,\n",
        "    num_classes=NUM_CLASSES\n",
        ").to(device)\n",
        "\n",
        "# Завантажуємо попередньо натреновану модель\n",
        "model_path = 'best_model.pth'\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(\"Модель завантажена з 'best_model.pth'\")\n",
        "else:\n",
        "    print(f\"Модель не знайдена за шляхом {model_path}. Навчання нової моделі...\")\n",
        "\n",
        "# Використовуємо класові ваги для оптимізації\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# ------------------------- Тренування моделі -------------------------\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for batch in final_loader:\n",
        "        if batch is None:\n",
        "            continue\n",
        "        images, input_ids, attention_mask, labels = batch\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Обмеження градієнта для стабільності\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    # ------------------------- Розрахунок QWK -------------------------\n",
        "    qwk = quadratic_weighted_kappa(all_labels, all_preds)\n",
        "    avg_loss = running_loss / len(final_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, QWK: {qwk:.4f}\")\n",
        "\n",
        "    # Очищення пам'яті GPU після кожної епохи\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Зберігаємо модель після тренування\n",
        "torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(\"Модель навчена та збережена!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_eycEuuUCSL",
        "outputId": "c8fc0c98-0cb5-48c0-9fe1-def0a393bc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing or empty text for PetID b5ff2b987, skipping\n",
            "Missing or empty text for PetID 480f33a74, skipping\n",
            "Missing or empty text for PetID ca659fd81, skipping\n",
            "Missing or empty text for PetID 937c2e14f, skipping\n",
            "Missing or empty text for PetID 0b3b23c20, skipping\n",
            "Missing or empty text for PetID 16504b328, skipping\n",
            "Missing or empty text for PetID 49a832741, skipping\n",
            "Missing or empty text for PetID fac19639c, skipping\n",
            "Модель завантажена з 'best_model.pth'\n",
            "Epoch [1/10], Loss: 0.7238, QWK: 0.8344\n",
            "Epoch [2/10], Loss: 0.6167, QWK: 0.8667\n",
            "Epoch [3/10], Loss: 0.5267, QWK: 0.8855\n",
            "Epoch [4/10], Loss: 0.4675, QWK: 0.8922\n",
            "Epoch [5/10], Loss: 0.4271, QWK: 0.9007\n",
            "Epoch [6/10], Loss: 0.4258, QWK: 0.8923\n",
            "Epoch [7/10], Loss: 0.3249, QWK: 0.9212\n",
            "Epoch [8/10], Loss: 0.2933, QWK: 0.9264\n",
            "Epoch [9/10], Loss: 0.2656, QWK: 0.9432\n",
            "Epoch [10/10], Loss: 0.2702, QWK: 0.9296\n",
            "Модель навчена та збережена!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- Завантаження тестових даних -------------------------\n",
        "test_df = pd.read_csv('/content/data/test.csv')\n",
        "\n",
        "# ------------------------- Токенізація текстів для тесту -------------------------\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    test_df['Description'].fillna(\"\").tolist(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=MAX_TEXT_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# ------------------------- Створення тестового Dataset -------------------------\n",
        "test_dataset = PetDataset(\n",
        "    test_df,\n",
        "    test_encodings['input_ids'],  # Окремо передаємо input_ids\n",
        "    image_dir='/content/data/images/images/test',\n",
        "    transform=image_transforms,  # Додаємо трансформації для зображень\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# DataLoader для тестового набору\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "# ------------------------- Завантаження попередньо навченої моделі -------------------------\n",
        "model = PetAdoptionModel(\n",
        "    hidden_size=hidden_size,\n",
        "    num_attention_heads=num_attention_heads,\n",
        "    num_hidden_layers=num_hidden_layers,\n",
        "    num_classes=NUM_CLASSES\n",
        ").to(device)\n",
        "\n",
        "# Завантажуємо збережену модель\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# ------------------------- Передбачення на тестових даних -------------------------\n",
        "predictions = []\n",
        "\n",
        "# Перевіримо, чи всі PetID мають зображення та текст\n",
        "for idx, data in enumerate(test_loader):\n",
        "    if data is None:\n",
        "        predictions.append(0)  # Якщо дані відсутні, додаємо найнижчий рейтинг\n",
        "        continue\n",
        "\n",
        "    images, input_ids, attention_mask, labels = data\n",
        "    if images is None or input_ids is None or attention_mask is None:\n",
        "        predictions.append(0)  # Якщо відсутні зображення або текст, присвоюємо найнижчий рейтинг\n",
        "        continue\n",
        "\n",
        "    images = images.to(device)\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # Передбачення\n",
        "    outputs = model(images, input_ids, attention_mask)\n",
        "    _, predicted_classes = torch.max(outputs, 1)\n",
        "\n",
        "    predictions.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "# Перевірка, чи є передбачення\n",
        "if len(predictions) != len(test_df):\n",
        "    # Якщо кількість передбачень менша, ніж кількість записів у тестовому наборі\n",
        "    print(f\"Warning: Expected {len(test_df)} predictions, but got {len(predictions)} predictions.\")\n",
        "    # Заповнюємо пропущені передбачення найнижчим рейтингом\n",
        "    missing_count = len(test_df) - len(predictions)\n",
        "    predictions.extend([0] * missing_count)  # Додаємо найнижчий рейтинг (0)\n",
        "\n",
        "# Формування submission файлу\n",
        "submission_df = pd.DataFrame({\n",
        "    'PetID': test_df['PetID'],\n",
        "    'AdoptionSpeed': predictions\n",
        "})\n",
        "\n",
        "# Виведемо, щоб переконатися, що файл збережено\n",
        "submission_file = '/content/data/submission.csv'\n",
        "\n",
        "# Збереження файлу\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Файл {submission_file} збережено!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLEZs5m4VfjB",
        "outputId": "31a3610b-6448-47fb-d771-2cf52f2b0a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No image found for PetID 95314294, skipping this data point.\n",
            "Warning: No image found for PetID 35992662, skipping this data point.\n",
            "Warning: No image found for PetID 63521459, skipping this data point.\n",
            "Warning: No image found for PetID 81301773, skipping this data point.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing or empty text for PetID ba16888d7, skipping\n",
            "Warning: Expected 1891 predictions, but got 1886 predictions.\n",
            "Файл /content/data/submission.csv збережено!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перевірка для кожного PetID на наявність зображень\n",
        "missing_images = []\n",
        "for pet_id in ['95314294', '35992662', '63521459', '81301773']:  # Перелічуємо PetID, для яких були попередження\n",
        "    image_found = False\n",
        "    for subfolder in ['train', 'test']:\n",
        "        image_path = os.path.join(image_dir, subfolder, f'{pet_id}-1.jpg')\n",
        "        if os.path.exists(image_path):\n",
        "            image_found = True\n",
        "            print(f\"Зображення знайдено для PetID {pet_id} за шляхом {image_path}\")\n",
        "            break  # Якщо зображення знайдено, не шукаємо більше\n",
        "\n",
        "    if not image_found:\n",
        "        missing_images.append(pet_id)\n",
        "        print(f\"Не знайдено зображення для PetID {pet_id}, перевірте шлях {image_path}\")\n",
        "\n",
        "# Перевірка, які PetID мають відсутні зображення\n",
        "if missing_images:\n",
        "    print(\"Перелік PetID без зображень:\", missing_images)\n",
        "else:\n",
        "    print(\"Усі зазначені PetID мають зображення.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gICqqFAs06Py",
        "outputId": "c1c5da8a-886e-4f1a-a300-22d6169f5ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Не знайдено зображення для PetID 95314294, перевірте шлях /content/data/images/images/test/95314294-1.jpg\n",
            "Не знайдено зображення для PetID 35992662, перевірте шлях /content/data/images/images/test/35992662-1.jpg\n",
            "Не знайдено зображення для PetID 63521459, перевірте шлях /content/data/images/images/test/63521459-1.jpg\n",
            "Не знайдено зображення для PetID 81301773, перевірте шлях /content/data/images/images/test/81301773-1.jpg\n",
            "Перелік PetID без зображень: ['95314294', '35992662', '63521459', '81301773']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "s7V36vjAarce",
        "outputId": "4baa7624-fda0-439f-f754-ba04e84d4a9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_722f826b-8856-4993-b156-3edf31491e83\", \"submission.csv\", 22708)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/data/submission.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b972c965d161462594e077f9c6761ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de29152d70624055a4ce5a6f61caa3bb",
              "IPY_MODEL_93a3985f99a84be1a82ad466e0f61570",
              "IPY_MODEL_bc416ae258874d63b05204ccee2546a6"
            ],
            "layout": "IPY_MODEL_f41814a778be4b969f9fdf208d898f86"
          }
        },
        "de29152d70624055a4ce5a6f61caa3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b93de66703c545c7adeaa2b43c4df563",
            "placeholder": "​",
            "style": "IPY_MODEL_5c79c83aa8ce4a128f7c4202085410c1",
            "value": "config.json: 100%"
          }
        },
        "93a3985f99a84be1a82ad466e0f61570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_998020b7723940e58d887b6399722676",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00d55b560b984167bf4ec282372d59c4",
            "value": 570
          }
        },
        "bc416ae258874d63b05204ccee2546a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ee7ed0aa014e12b9c8666b74e7b710",
            "placeholder": "​",
            "style": "IPY_MODEL_c982ed59731e4c2eafed3fc153eeafb7",
            "value": " 570/570 [00:00&lt;00:00, 44.8kB/s]"
          }
        },
        "f41814a778be4b969f9fdf208d898f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b93de66703c545c7adeaa2b43c4df563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c79c83aa8ce4a128f7c4202085410c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "998020b7723940e58d887b6399722676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d55b560b984167bf4ec282372d59c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39ee7ed0aa014e12b9c8666b74e7b710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c982ed59731e4c2eafed3fc153eeafb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895782f530654dcda16e604498a52517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f83f0005bf4a4d0085fa0997de1e2531",
              "IPY_MODEL_756637439b7b4497b8163cc55e5d0aea",
              "IPY_MODEL_6fe91a0558984c39a6d500c506c8627a"
            ],
            "layout": "IPY_MODEL_7af200a4dbc44cb0b3c9f90fe6769706"
          }
        },
        "f83f0005bf4a4d0085fa0997de1e2531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d296d26bfe4cb694ca8e26fa92b181",
            "placeholder": "​",
            "style": "IPY_MODEL_9419235cff00446d90391ca14261a4b8",
            "value": "model.safetensors: 100%"
          }
        },
        "756637439b7b4497b8163cc55e5d0aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48aa82fd75474efd9d6a8cebbe38c03a",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65b66750977a4fc79c82e3d2e07f8a08",
            "value": 440449768
          }
        },
        "6fe91a0558984c39a6d500c506c8627a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b1ee238fc944abb6fa479311e66b51",
            "placeholder": "​",
            "style": "IPY_MODEL_36b9244ca57846b5ace54f9fbc63a7e4",
            "value": " 440M/440M [00:02&lt;00:00, 233MB/s]"
          }
        },
        "7af200a4dbc44cb0b3c9f90fe6769706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82d296d26bfe4cb694ca8e26fa92b181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9419235cff00446d90391ca14261a4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48aa82fd75474efd9d6a8cebbe38c03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b66750977a4fc79c82e3d2e07f8a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00b1ee238fc944abb6fa479311e66b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b9244ca57846b5ace54f9fbc63a7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b2a3ed140f450982f518ee91912fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d75714cb24874195bfd368a08d925f63",
              "IPY_MODEL_390405f67dcf406f92071f9239d29d3f",
              "IPY_MODEL_ca4a845fffdc4610b7f8e6f38a7dc159"
            ],
            "layout": "IPY_MODEL_ab0e4a69964549f5a416cf4b90cd272b"
          }
        },
        "d75714cb24874195bfd368a08d925f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e45000f4706e40caae982e99f9bc4f7d",
            "placeholder": "​",
            "style": "IPY_MODEL_70eb67e4003c4328a9683b07730e303c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "390405f67dcf406f92071f9239d29d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660bdc8c0b1f4b51986e3396face4168",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6c0f155b02a425f95437fa30beafbb5",
            "value": 48
          }
        },
        "ca4a845fffdc4610b7f8e6f38a7dc159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeb14507576b4d71ad5e0d490f51f757",
            "placeholder": "​",
            "style": "IPY_MODEL_c5ce1a9681a24570b94bda70c5e93790",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.52kB/s]"
          }
        },
        "ab0e4a69964549f5a416cf4b90cd272b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45000f4706e40caae982e99f9bc4f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70eb67e4003c4328a9683b07730e303c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "660bdc8c0b1f4b51986e3396face4168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c0f155b02a425f95437fa30beafbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eeb14507576b4d71ad5e0d490f51f757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ce1a9681a24570b94bda70c5e93790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1df5b7383e4d4bf493ea69e3238c92a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7969cdbd914c4824af40a553270f146d",
              "IPY_MODEL_1f891850eb0c40d88160fc6eca0a8078",
              "IPY_MODEL_b38d26b070734be1b773f82cf005571b"
            ],
            "layout": "IPY_MODEL_56eea96767e647afb4e2b735b86f449f"
          }
        },
        "7969cdbd914c4824af40a553270f146d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c36e585b9e24ce1a824035e9da9e4b2",
            "placeholder": "​",
            "style": "IPY_MODEL_8d373867a71d4ab2b746e570c2aca66a",
            "value": "vocab.txt: 100%"
          }
        },
        "1f891850eb0c40d88160fc6eca0a8078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69ea8eb9e817457885fa1e58249b665b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be29716d5d874214a0a1ee481611a851",
            "value": 231508
          }
        },
        "b38d26b070734be1b773f82cf005571b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ba63bd79b94bfeba89c14d128ea08e",
            "placeholder": "​",
            "style": "IPY_MODEL_c203d0a55daf4f0c8ac8678acb8f84e4",
            "value": " 232k/232k [00:00&lt;00:00, 3.22MB/s]"
          }
        },
        "56eea96767e647afb4e2b735b86f449f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c36e585b9e24ce1a824035e9da9e4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d373867a71d4ab2b746e570c2aca66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ea8eb9e817457885fa1e58249b665b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be29716d5d874214a0a1ee481611a851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1ba63bd79b94bfeba89c14d128ea08e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c203d0a55daf4f0c8ac8678acb8f84e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f27a5e6c80747838cd679e69f550032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a39c17c0a6848bca4d39bbdc8dc3085",
              "IPY_MODEL_643424c5a3f6489189273bffcd3489fe",
              "IPY_MODEL_1bc5fd00cdad4df5ae36fc5c02ec53cc"
            ],
            "layout": "IPY_MODEL_001a242e68734b4fbe442b668cf7c061"
          }
        },
        "6a39c17c0a6848bca4d39bbdc8dc3085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4102314cd9414d09b8db882a09fa0294",
            "placeholder": "​",
            "style": "IPY_MODEL_bf9f9f25a3c1428ba148d3c50f506bbe",
            "value": "tokenizer.json: 100%"
          }
        },
        "643424c5a3f6489189273bffcd3489fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9f6b6b19f0c4bf5bdab4daab03b5c0d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_876c750b07814f43b238d086b78ffe05",
            "value": 466062
          }
        },
        "1bc5fd00cdad4df5ae36fc5c02ec53cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a64505bb5a74aadb55cfe1fb245ab3b",
            "placeholder": "​",
            "style": "IPY_MODEL_85cd4006635449ae86912a8f9f36dedd",
            "value": " 466k/466k [00:00&lt;00:00, 6.30MB/s]"
          }
        },
        "001a242e68734b4fbe442b668cf7c061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4102314cd9414d09b8db882a09fa0294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9f9f25a3c1428ba148d3c50f506bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9f6b6b19f0c4bf5bdab4daab03b5c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876c750b07814f43b238d086b78ffe05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a64505bb5a74aadb55cfe1fb245ab3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85cd4006635449ae86912a8f9f36dedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}